{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model from scratch\n",
    "\n",
    "Here we create our own Vocab and iterator without using torchtext or any other library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOsxmkJC6XV7"
   },
   "outputs": [],
   "source": [
    "!wget -q https://github.com/pytorch/examples/raw/master/word_language_model/data/wikitext-2/train.txt\n",
    "!wget -q https://github.com/pytorch/examples/raw/master/word_language_model/data/wikitext-2/test.txt\n",
    "!wget -q https://github.com/pytorch/examples/raw/master/word_language_model/data/wikitext-2/valid.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3aWbO3PH6nxE",
    "outputId": "ab7da20e-cf0b-4465-c40c-f2f7282e8ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data  test.txt  train.txt  valid.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZuHpILs6oMY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from io import open\n",
    "from pathlib import Path\n",
    "import  torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YODesUMhJD7I"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "244fDnwv9Kbh"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    \"\"\"The class which holds the mapping from word2idx and idx2word.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx={}\n",
    "        self.idx2word=[]\n",
    "    \n",
    "    def add_word(self,word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word]= len(self.idx2word)-1\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "    \n",
    "class Corpus(object):\n",
    "    \"\"\"The class which holds all the three data sets.\n",
    "    We maintain on Single vocab for all train, test, val.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,path):\n",
    "        self.dictionary = Dictionary()\n",
    "        \n",
    "        self.train = self.tokenize(path/\"train.txt\") # tokenize the data\n",
    "        self.test  = self.tokenize(path/\"valid.txt\")\n",
    "        self.valid = self.tokenize(path/\"test.txt\")\n",
    "        return None\n",
    "    def tokenize(self,path):\n",
    "        \n",
    "        with open(path,\"r\",encoding=\"utf8\") as f:\n",
    "            tokens=0\n",
    "            for line in f:\n",
    "                words = line.split() + [\"<eos>\"]\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                    \n",
    "        #tokenize file content\n",
    "        with open(path , \"r\" , encoding=\"utf8\") as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token=0\n",
    "            for line in f:\n",
    "                words = line.split() +[\"<eos>\"]\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token+=1\n",
    "                    \n",
    "        \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8OwaYVdKdM-"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,rnn_type,\n",
    "                 ntoken,\n",
    "                 ninp,\n",
    "                 nhid ,\n",
    "                 nlayers , \n",
    "                 dropout =0.5):\n",
    "        super(RNNModel,self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken,ninp)\n",
    "        self.rnn = getattr(nn,rnn_type)(ninp,nhid,nlayers,dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid,ntoken)\n",
    "        self.init_weights()\n",
    "        self.rnn_type=rnn_type\n",
    "        self.nhid=nhid\n",
    "        self.nlayers=nlayers\n",
    "        return None\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange,initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange,initrange)\n",
    "    \n",
    "    def forward(self,input,hidden):\n",
    "        emb  = self.drop(self.encoder(input))\n",
    "        output,hidden = self.rnn(emb,hidden)\n",
    "        print(output.shape)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.shape[0]*output.shape[1] , output.shape[2]))\n",
    "        return decoded.view(output.size(0),output.size(1),decoded.size(1)) , hidden\n",
    "    \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G68Zszm6huyN"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "corpus = Corpus(Path(\"./\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIX91-GakYcF"
   },
   "outputs": [],
   "source": [
    "def batchify(data , bsz):\n",
    "    \n",
    "    nbatch = data.size(0) // bsz\n",
    "    \n",
    "    data = data.narrow(0,0,nbatch*bsz)\n",
    "    \n",
    "    data = data.view(bsz,-1).t().contiguous()\n",
    "    \n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Xx1z9rklFxc"
   },
   "outputs": [],
   "source": [
    "eval_batch_size = 10\n",
    "train_batch_size = 20\n",
    "train_data = batchify(corpus.train, train_batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Yt0SRtNNGufS",
    "outputId": "fa16166e-b78a-496a-d8fc-94756f053068"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([104431, 20]), torch.Size([2088628]))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OgpZtvClJUt"
   },
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FO98g-ErmT3C"
   },
   "outputs": [],
   "source": [
    "model = RNNModel(\"LSTM\", ntokens, ninp = 300, nhid=200, nlayers=2,dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKzerdMAnRaL"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "9zV_XPntnfLH",
    "outputId": "93f3a834-e205-4299-abb7-ac333b053957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,   284, 15178,   280,   348,   128,   289,  9493,    16,     1],\n",
       "         [    1,   357,    43,  2977,   530, 23080,    13,    78,    17,     0],\n",
       "         [    2,  1496,  7369,   115,  4782,    37, 22196,   252, 26998,     0]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,   357,    43,  2977,   530, 23080,    13,    78,    17,     0,\n",
       "           4312,     0,   151,    22, 18215,    17,    17,    46,    43,  2015],\n",
       "         [    2,  1496,  7369,   115,  4782,    37, 22196,   252, 26998,     0,\n",
       "          28680,     1,   496,  2193,  1037,     9,  4072,   380,    27, 33001],\n",
       "         [    3,   449,   310,     9,    13,  8034,  3107,   639,    13, 27958,\n",
       "            638,     1,   168,    17,    43,  2786,    15,   160,   152,  3072]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bptt=3\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target\n",
    "data , target = get_batch(train_data,0)\n",
    "data[:,:10] , target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8XPyNO3rxBv"
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226/7\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7e4qZq9pzod"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(bsz=20)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, 35)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 200 == 0 and batch > 0:\n",
    "            cur_loss = total_loss / 200\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // 35, lr,\n",
    "                elapsed * 1000 / 200, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUcXCk4mp-qM"
   },
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, 35):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11933
    },
    "colab_type": "code",
    "id": "dFgBT1sztzse",
    "outputId": "bffbf9e6-3f3c-416d-bbc9-f101644714b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 21.31 | loss  7.62 | ppl  2034.50\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 20.33 | loss  6.83 | ppl   923.59\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 20.32 | loss  6.47 | ppl   642.53\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 20.34 | loss  6.27 | ppl   526.87\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 20.36 | loss  6.11 | ppl   452.09\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 20.32 | loss  6.04 | ppl   418.94\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 20.32 | loss  5.92 | ppl   372.74\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 20.30 | loss  5.94 | ppl   378.09\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 20.30 | loss  5.78 | ppl   325.18\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 20.29 | loss  5.75 | ppl   313.59\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 20.29 | loss  5.65 | ppl   282.97\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 20.29 | loss  5.65 | ppl   285.21\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 20.33 | loss  5.64 | ppl   280.33\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 20.41 | loss  5.53 | ppl   251.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 63.59s | valid loss  5.42 | valid ppl   225.24\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 20.85 | loss  5.53 | ppl   252.97\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 20.83 | loss  5.51 | ppl   247.14\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 21.04 | loss  5.34 | ppl   207.61\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 20.74 | loss  5.35 | ppl   211.40\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 20.72 | loss  5.33 | ppl   206.64\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 20.71 | loss  5.31 | ppl   202.71\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 20.90 | loss  5.31 | ppl   202.72\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 21.10 | loss  5.37 | ppl   215.36\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 21.35 | loss  5.24 | ppl   188.17\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 21.49 | loss  5.25 | ppl   190.15\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 21.63 | loss  5.15 | ppl   172.70\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 21.66 | loss  5.19 | ppl   178.79\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 21.68 | loss  5.20 | ppl   180.88\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 21.56 | loss  5.12 | ppl   166.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 65.92s | valid loss  5.18 | valid ppl   176.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 21.56 | loss  5.17 | ppl   176.63\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 21.31 | loss  5.18 | ppl   177.80\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 21.40 | loss  5.00 | ppl   147.71\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 21.42 | loss  5.05 | ppl   155.55\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 21.43 | loss  5.03 | ppl   153.05\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 21.51 | loss  5.03 | ppl   153.32\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 21.46 | loss  5.06 | ppl   157.62\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 21.56 | loss  5.13 | ppl   168.41\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 21.54 | loss  4.99 | ppl   147.38\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 21.59 | loss  5.02 | ppl   151.29\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 21.61 | loss  4.92 | ppl   137.22\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 21.79 | loss  4.96 | ppl   142.95\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 21.66 | loss  4.98 | ppl   145.51\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 21.71 | loss  4.91 | ppl   135.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 67.02s | valid loss  5.10 | valid ppl   163.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 21.82 | loss  4.97 | ppl   144.41\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 21.61 | loss  4.99 | ppl   147.45\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 21.75 | loss  4.81 | ppl   122.39\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 21.74 | loss  4.87 | ppl   130.39\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 21.64 | loss  4.86 | ppl   129.37\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 21.70 | loss  4.87 | ppl   130.13\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 21.68 | loss  4.91 | ppl   135.22\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 21.71 | loss  4.98 | ppl   145.31\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 21.72 | loss  4.85 | ppl   127.78\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 21.75 | loss  4.88 | ppl   131.24\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 21.90 | loss  4.78 | ppl   118.78\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 21.91 | loss  4.82 | ppl   123.71\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 21.77 | loss  4.85 | ppl   127.18\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 21.84 | loss  4.77 | ppl   118.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 67.81s | valid loss  5.02 | valid ppl   150.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 21.95 | loss  4.84 | ppl   126.20\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 21.75 | loss  4.87 | ppl   130.20\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 21.98 | loss  4.68 | ppl   107.87\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 21.95 | loss  4.75 | ppl   115.32\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 21.93 | loss  4.75 | ppl   115.26\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 21.92 | loss  4.75 | ppl   115.69\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 21.92 | loss  4.80 | ppl   121.81\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 21.88 | loss  4.88 | ppl   130.99\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 21.98 | loss  4.75 | ppl   115.02\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 22.06 | loss  4.78 | ppl   119.24\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 22.05 | loss  4.67 | ppl   107.09\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 22.00 | loss  4.71 | ppl   111.14\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 21.98 | loss  4.74 | ppl   114.94\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 22.07 | loss  4.67 | ppl   106.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 68.28s | valid loss  4.97 | valid ppl   144.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 20.00 | ms/batch 22.25 | loss  4.74 | ppl   114.84\n",
      "| epoch   6 |   400/ 2983 batches | lr 20.00 | ms/batch 22.08 | loss  4.77 | ppl   117.83\n",
      "| epoch   6 |   600/ 2983 batches | lr 20.00 | ms/batch 22.12 | loss  4.59 | ppl    98.22\n",
      "| epoch   6 |   800/ 2983 batches | lr 20.00 | ms/batch 22.12 | loss  4.65 | ppl   104.61\n",
      "| epoch   6 |  1000/ 2983 batches | lr 20.00 | ms/batch 22.12 | loss  4.66 | ppl   105.45\n",
      "| epoch   6 |  1200/ 2983 batches | lr 20.00 | ms/batch 22.17 | loss  4.67 | ppl   106.28\n",
      "| epoch   6 |  1400/ 2983 batches | lr 20.00 | ms/batch 22.12 | loss  4.71 | ppl   110.73\n",
      "| epoch   6 |  1600/ 2983 batches | lr 20.00 | ms/batch 22.05 | loss  4.78 | ppl   119.54\n",
      "| epoch   6 |  1800/ 2983 batches | lr 20.00 | ms/batch 22.12 | loss  4.67 | ppl   106.19\n",
      "| epoch   6 |  2000/ 2983 batches | lr 20.00 | ms/batch 22.16 | loss  4.70 | ppl   109.62\n",
      "| epoch   6 |  2200/ 2983 batches | lr 20.00 | ms/batch 22.26 | loss  4.59 | ppl    98.28\n",
      "| epoch   6 |  2400/ 2983 batches | lr 20.00 | ms/batch 22.15 | loss  4.64 | ppl   103.16\n",
      "| epoch   6 |  2600/ 2983 batches | lr 20.00 | ms/batch 22.09 | loss  4.66 | ppl   106.06\n",
      "| epoch   6 |  2800/ 2983 batches | lr 20.00 | ms/batch 22.21 | loss  4.60 | ppl    99.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 68.81s | valid loss  4.93 | valid ppl   138.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2983 batches | lr 20.00 | ms/batch 22.23 | loss  4.66 | ppl   105.37\n",
      "| epoch   7 |   400/ 2983 batches | lr 20.00 | ms/batch 22.21 | loss  4.69 | ppl   108.79\n",
      "| epoch   7 |   600/ 2983 batches | lr 20.00 | ms/batch 22.34 | loss  4.51 | ppl    90.86\n",
      "| epoch   7 |   800/ 2983 batches | lr 20.00 | ms/batch 22.23 | loss  4.58 | ppl    97.45\n",
      "| epoch   7 |  1000/ 2983 batches | lr 20.00 | ms/batch 22.28 | loss  4.59 | ppl    98.88\n",
      "| epoch   7 |  1200/ 2983 batches | lr 20.00 | ms/batch 22.24 | loss  4.60 | ppl    99.70\n",
      "| epoch   7 |  1400/ 2983 batches | lr 20.00 | ms/batch 22.23 | loss  4.64 | ppl   103.31\n",
      "| epoch   7 |  1600/ 2983 batches | lr 20.00 | ms/batch 22.17 | loss  4.71 | ppl   111.55\n",
      "| epoch   7 |  1800/ 2983 batches | lr 20.00 | ms/batch 22.19 | loss  4.60 | ppl    99.53\n",
      "| epoch   7 |  2000/ 2983 batches | lr 20.00 | ms/batch 22.22 | loss  4.63 | ppl   102.59\n",
      "| epoch   7 |  2200/ 2983 batches | lr 20.00 | ms/batch 22.23 | loss  4.52 | ppl    91.96\n",
      "| epoch   7 |  2400/ 2983 batches | lr 20.00 | ms/batch 22.23 | loss  4.56 | ppl    95.95\n",
      "| epoch   7 |  2600/ 2983 batches | lr 20.00 | ms/batch 22.17 | loss  4.60 | ppl    99.64\n",
      "| epoch   7 |  2800/ 2983 batches | lr 20.00 | ms/batch 22.19 | loss  4.54 | ppl    93.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 69.04s | valid loss  4.93 | valid ppl   137.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2983 batches | lr 20.00 | ms/batch 22.33 | loss  4.60 | ppl    98.99\n",
      "| epoch   8 |   400/ 2983 batches | lr 20.00 | ms/batch 22.28 | loss  4.63 | ppl   102.42\n",
      "| epoch   8 |   600/ 2983 batches | lr 20.00 | ms/batch 22.20 | loss  4.45 | ppl    85.52\n",
      "| epoch   8 |   800/ 2983 batches | lr 20.00 | ms/batch 22.25 | loss  4.51 | ppl    91.22\n",
      "| epoch   8 |  1000/ 2983 batches | lr 20.00 | ms/batch 22.27 | loss  4.53 | ppl    93.08\n",
      "| epoch   8 |  1200/ 2983 batches | lr 20.00 | ms/batch 22.27 | loss  4.54 | ppl    94.03\n",
      "| epoch   8 |  1400/ 2983 batches | lr 20.00 | ms/batch 22.35 | loss  4.58 | ppl    97.97\n",
      "| epoch   8 |  1600/ 2983 batches | lr 20.00 | ms/batch 22.32 | loss  4.66 | ppl   105.65\n",
      "| epoch   8 |  1800/ 2983 batches | lr 20.00 | ms/batch 22.26 | loss  4.55 | ppl    94.25\n",
      "| epoch   8 |  2000/ 2983 batches | lr 20.00 | ms/batch 22.29 | loss  4.58 | ppl    97.05\n",
      "| epoch   8 |  2200/ 2983 batches | lr 20.00 | ms/batch 22.27 | loss  4.47 | ppl    87.34\n",
      "| epoch   8 |  2400/ 2983 batches | lr 20.00 | ms/batch 22.26 | loss  4.51 | ppl    91.13\n",
      "| epoch   8 |  2600/ 2983 batches | lr 20.00 | ms/batch 22.27 | loss  4.54 | ppl    93.92\n",
      "| epoch   8 |  2800/ 2983 batches | lr 20.00 | ms/batch 22.31 | loss  4.49 | ppl    89.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 69.19s | valid loss  4.94 | valid ppl   139.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2983 batches | lr 5.00 | ms/batch 22.29 | loss  4.59 | ppl    98.80\n",
      "| epoch   9 |   400/ 2983 batches | lr 5.00 | ms/batch 22.16 | loss  4.59 | ppl    98.11\n",
      "| epoch   9 |   600/ 2983 batches | lr 5.00 | ms/batch 22.20 | loss  4.40 | ppl    81.44\n",
      "| epoch   9 |   800/ 2983 batches | lr 5.00 | ms/batch 22.19 | loss  4.45 | ppl    85.56\n",
      "| epoch   9 |  1000/ 2983 batches | lr 5.00 | ms/batch 22.26 | loss  4.44 | ppl    84.69\n",
      "| epoch   9 |  1200/ 2983 batches | lr 5.00 | ms/batch 22.28 | loss  4.43 | ppl    84.31\n",
      "| epoch   9 |  1400/ 2983 batches | lr 5.00 | ms/batch 22.21 | loss  4.45 | ppl    85.44\n",
      "| epoch   9 |  1600/ 2983 batches | lr 5.00 | ms/batch 22.24 | loss  4.51 | ppl    90.84\n",
      "| epoch   9 |  1800/ 2983 batches | lr 5.00 | ms/batch 22.31 | loss  4.37 | ppl    79.15\n",
      "| epoch   9 |  2000/ 2983 batches | lr 5.00 | ms/batch 22.29 | loss  4.40 | ppl    81.31\n",
      "| epoch   9 |  2200/ 2983 batches | lr 5.00 | ms/batch 22.31 | loss  4.27 | ppl    71.70\n",
      "| epoch   9 |  2400/ 2983 batches | lr 5.00 | ms/batch 22.24 | loss  4.29 | ppl    73.19\n",
      "| epoch   9 |  2600/ 2983 batches | lr 5.00 | ms/batch 22.23 | loss  4.31 | ppl    74.37\n",
      "| epoch   9 |  2800/ 2983 batches | lr 5.00 | ms/batch 22.22 | loss  4.24 | ppl    69.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 69.10s | valid loss  4.77 | valid ppl   118.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2983 batches | lr 5.00 | ms/batch 22.35 | loss  4.44 | ppl    84.56\n",
      "| epoch  10 |   400/ 2983 batches | lr 5.00 | ms/batch 22.13 | loss  4.45 | ppl    85.95\n",
      "| epoch  10 |   600/ 2983 batches | lr 5.00 | ms/batch 22.26 | loss  4.27 | ppl    71.83\n",
      "| epoch  10 |   800/ 2983 batches | lr 5.00 | ms/batch 22.18 | loss  4.34 | ppl    76.74\n",
      "| epoch  10 |  1000/ 2983 batches | lr 5.00 | ms/batch 22.24 | loss  4.34 | ppl    76.37\n",
      "| epoch  10 |  1200/ 2983 batches | lr 5.00 | ms/batch 22.28 | loss  4.34 | ppl    77.07\n",
      "| epoch  10 |  1400/ 2983 batches | lr 5.00 | ms/batch 22.26 | loss  4.37 | ppl    79.24\n",
      "| epoch  10 |  1600/ 2983 batches | lr 5.00 | ms/batch 22.26 | loss  4.44 | ppl    84.82\n",
      "| epoch  10 |  1800/ 2983 batches | lr 5.00 | ms/batch 22.27 | loss  4.31 | ppl    74.44\n",
      "| epoch  10 |  2000/ 2983 batches | lr 5.00 | ms/batch 22.23 | loss  4.34 | ppl    76.97\n",
      "| epoch  10 |  2200/ 2983 batches | lr 5.00 | ms/batch 22.26 | loss  4.22 | ppl    68.20\n",
      "| epoch  10 |  2400/ 2983 batches | lr 5.00 | ms/batch 22.31 | loss  4.25 | ppl    70.19\n",
      "| epoch  10 |  2600/ 2983 batches | lr 5.00 | ms/batch 22.33 | loss  4.27 | ppl    71.49\n",
      "| epoch  10 |  2800/ 2983 batches | lr 5.00 | ms/batch 22.28 | loss  4.21 | ppl    67.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 69.14s | valid loss  4.76 | valid ppl   116.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 2983 batches | lr 5.00 | ms/batch 22.38 | loss  4.38 | ppl    80.06\n",
      "| epoch  11 |   400/ 2983 batches | lr 5.00 | ms/batch 22.30 | loss  4.40 | ppl    81.09\n",
      "| epoch  11 |   600/ 2983 batches | lr 5.00 | ms/batch 22.25 | loss  4.22 | ppl    68.09\n",
      "| epoch  11 |   800/ 2983 batches | lr 5.00 | ms/batch 22.23 | loss  4.29 | ppl    72.83\n",
      "| epoch  11 |  1000/ 2983 batches | lr 5.00 | ms/batch 22.27 | loss  4.29 | ppl    72.76\n",
      "| epoch  11 |  1200/ 2983 batches | lr 5.00 | ms/batch 22.25 | loss  4.30 | ppl    73.50\n",
      "| epoch  11 |  1400/ 2983 batches | lr 5.00 | ms/batch 22.24 | loss  4.33 | ppl    75.99\n",
      "| epoch  11 |  1600/ 2983 batches | lr 5.00 | ms/batch 22.24 | loss  4.40 | ppl    81.40\n",
      "| epoch  11 |  1800/ 2983 batches | lr 5.00 | ms/batch 22.29 | loss  4.27 | ppl    71.84\n",
      "| epoch  11 |  2000/ 2983 batches | lr 5.00 | ms/batch 22.28 | loss  4.30 | ppl    73.76\n",
      "| epoch  11 |  2200/ 2983 batches | lr 5.00 | ms/batch 22.33 | loss  4.19 | ppl    65.94\n",
      "| epoch  11 |  2400/ 2983 batches | lr 5.00 | ms/batch 22.28 | loss  4.22 | ppl    68.17\n",
      "| epoch  11 |  2600/ 2983 batches | lr 5.00 | ms/batch 22.28 | loss  4.24 | ppl    69.54\n",
      "| epoch  11 |  2800/ 2983 batches | lr 5.00 | ms/batch 22.24 | loss  4.19 | ppl    65.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 69.34s | valid loss  4.75 | valid ppl   115.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 2983 batches | lr 5.00 | ms/batch 22.30 | loss  4.34 | ppl    76.45\n",
      "| epoch  12 |   400/ 2983 batches | lr 5.00 | ms/batch 22.29 | loss  4.35 | ppl    77.74\n",
      "| epoch  12 |   600/ 2983 batches | lr 5.00 | ms/batch 22.17 | loss  4.18 | ppl    65.55\n",
      "| epoch  12 |   800/ 2983 batches | lr 5.00 | ms/batch 22.18 | loss  4.24 | ppl    69.71\n",
      "| epoch  12 |  1000/ 2983 batches | lr 5.00 | ms/batch 22.27 | loss  4.25 | ppl    70.38\n",
      "| epoch  12 |  1200/ 2983 batches | lr 5.00 | ms/batch 22.26 | loss  4.27 | ppl    71.22\n",
      "| epoch  12 |  1400/ 2983 batches | lr 5.00 | ms/batch 22.20 | loss  4.30 | ppl    73.38\n",
      "| epoch  12 |  1600/ 2983 batches | lr 5.00 | ms/batch 22.27 | loss  4.37 | ppl    78.79\n",
      "| epoch  12 |  1800/ 2983 batches | lr 5.00 | ms/batch 22.23 | loss  4.25 | ppl    69.89\n",
      "| epoch  12 |  2000/ 2983 batches | lr 5.00 | ms/batch 22.22 | loss  4.27 | ppl    71.88\n",
      "| epoch  12 |  2200/ 2983 batches | lr 5.00 | ms/batch 22.25 | loss  4.16 | ppl    64.32\n",
      "| epoch  12 |  2400/ 2983 batches | lr 5.00 | ms/batch 22.25 | loss  4.20 | ppl    66.71\n",
      "| epoch  12 |  2600/ 2983 batches | lr 5.00 | ms/batch 22.27 | loss  4.23 | ppl    68.45\n",
      "| epoch  12 |  2800/ 2983 batches | lr 5.00 | ms/batch 22.18 | loss  4.16 | ppl    64.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 69.08s | valid loss  4.74 | valid ppl   114.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 2983 batches | lr 5.00 | ms/batch 22.33 | loss  4.30 | ppl    74.00\n",
      "| epoch  13 |   400/ 2983 batches | lr 5.00 | ms/batch 22.29 | loss  4.32 | ppl    75.52\n",
      "| epoch  13 |   600/ 2983 batches | lr 5.00 | ms/batch 22.23 | loss  4.15 | ppl    63.42\n",
      "| epoch  13 |   800/ 2983 batches | lr 5.00 | ms/batch 22.23 | loss  4.22 | ppl    67.74\n",
      "| epoch  13 |  1000/ 2983 batches | lr 5.00 | ms/batch 22.25 | loss  4.22 | ppl    68.18\n",
      "| epoch  13 |  1200/ 2983 batches | lr 5.00 | ms/batch 22.29 | loss  4.24 | ppl    69.38\n",
      "| epoch  13 |  1400/ 2983 batches | lr 5.00 | ms/batch 22.22 | loss  4.27 | ppl    71.17\n",
      "| epoch  13 |  1600/ 2983 batches | lr 5.00 | ms/batch 22.27 | loss  4.34 | ppl    76.48\n",
      "| epoch  13 |  1800/ 2983 batches | lr 5.00 | ms/batch 22.27 | loss  4.21 | ppl    67.66\n",
      "| epoch  13 |  2000/ 2983 batches | lr 5.00 | ms/batch 22.22 | loss  4.25 | ppl    69.99\n",
      "| epoch  13 |  2200/ 2983 batches | lr 5.00 | ms/batch 22.25 | loss  4.14 | ppl    62.87\n",
      "| epoch  13 |  2400/ 2983 batches | lr 5.00 | ms/batch 22.26 | loss  4.18 | ppl    65.08\n",
      "| epoch  13 |  2600/ 2983 batches | lr 5.00 | ms/batch 22.26 | loss  4.20 | ppl    66.65\n",
      "| epoch  13 |  2800/ 2983 batches | lr 5.00 | ms/batch 22.27 | loss  4.15 | ppl    63.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 69.14s | valid loss  4.75 | valid ppl   115.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 2983 batches | lr 1.25 | ms/batch 22.32 | loss  4.32 | ppl    75.45\n",
      "| epoch  14 |   400/ 2983 batches | lr 1.25 | ms/batch 22.21 | loss  4.34 | ppl    76.63\n",
      "| epoch  14 |   600/ 2983 batches | lr 1.25 | ms/batch 22.17 | loss  4.17 | ppl    64.81\n",
      "| epoch  14 |   800/ 2983 batches | lr 1.25 | ms/batch 22.27 | loss  4.24 | ppl    69.37\n",
      "| epoch  14 |  1000/ 2983 batches | lr 1.25 | ms/batch 22.32 | loss  4.23 | ppl    68.84\n",
      "| epoch  14 |  1200/ 2983 batches | lr 1.25 | ms/batch 22.31 | loss  4.24 | ppl    69.11\n",
      "| epoch  14 |  1400/ 2983 batches | lr 1.25 | ms/batch 22.21 | loss  4.26 | ppl    70.57\n",
      "| epoch  14 |  1600/ 2983 batches | lr 1.25 | ms/batch 22.23 | loss  4.32 | ppl    74.89\n",
      "| epoch  14 |  1800/ 2983 batches | lr 1.25 | ms/batch 22.16 | loss  4.19 | ppl    65.92\n",
      "| epoch  14 |  2000/ 2983 batches | lr 1.25 | ms/batch 22.20 | loss  4.22 | ppl    67.94\n",
      "| epoch  14 |  2200/ 2983 batches | lr 1.25 | ms/batch 22.19 | loss  4.09 | ppl    59.96\n",
      "| epoch  14 |  2400/ 2983 batches | lr 1.25 | ms/batch 22.25 | loss  4.13 | ppl    62.29\n",
      "| epoch  14 |  2600/ 2983 batches | lr 1.25 | ms/batch 22.21 | loss  4.15 | ppl    63.18\n",
      "| epoch  14 |  2800/ 2983 batches | lr 1.25 | ms/batch 22.22 | loss  4.08 | ppl    58.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 69.07s | valid loss  4.71 | valid ppl   110.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 2983 batches | lr 1.25 | ms/batch 22.40 | loss  4.29 | ppl    72.92\n",
      "| epoch  15 |   400/ 2983 batches | lr 1.25 | ms/batch 22.25 | loss  4.31 | ppl    74.14\n",
      "| epoch  15 |   600/ 2983 batches | lr 1.25 | ms/batch 22.33 | loss  4.13 | ppl    62.21\n",
      "| epoch  15 |   800/ 2983 batches | lr 1.25 | ms/batch 22.34 | loss  4.20 | ppl    66.74\n",
      "| epoch  15 |  1000/ 2983 batches | lr 1.25 | ms/batch 22.35 | loss  4.20 | ppl    66.41\n",
      "| epoch  15 |  1200/ 2983 batches | lr 1.25 | ms/batch 22.36 | loss  4.21 | ppl    67.21\n",
      "| epoch  15 |  1400/ 2983 batches | lr 1.25 | ms/batch 22.37 | loss  4.24 | ppl    69.10\n",
      "| epoch  15 |  1600/ 2983 batches | lr 1.25 | ms/batch 22.41 | loss  4.29 | ppl    73.13\n",
      "| epoch  15 |  1800/ 2983 batches | lr 1.25 | ms/batch 22.33 | loss  4.18 | ppl    65.05\n",
      "| epoch  15 |  2000/ 2983 batches | lr 1.25 | ms/batch 22.34 | loss  4.21 | ppl    67.16\n",
      "| epoch  15 |  2200/ 2983 batches | lr 1.25 | ms/batch 22.33 | loss  4.08 | ppl    59.12\n",
      "| epoch  15 |  2400/ 2983 batches | lr 1.25 | ms/batch 22.30 | loss  4.12 | ppl    61.56\n",
      "| epoch  15 |  2600/ 2983 batches | lr 1.25 | ms/batch 22.33 | loss  4.15 | ppl    63.22\n",
      "| epoch  15 |  2800/ 2983 batches | lr 1.25 | ms/batch 22.32 | loss  4.08 | ppl    59.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 69.38s | valid loss  4.70 | valid ppl   110.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 2983 batches | lr 1.25 | ms/batch 22.36 | loss  4.27 | ppl    71.57\n",
      "| epoch  16 |   400/ 2983 batches | lr 1.25 | ms/batch 22.37 | loss  4.28 | ppl    72.43\n",
      "| epoch  16 |   600/ 2983 batches | lr 1.25 | ms/batch 22.35 | loss  4.11 | ppl    60.93\n",
      "| epoch  16 |   800/ 2983 batches | lr 1.25 | ms/batch 22.34 | loss  4.18 | ppl    65.58\n",
      "| epoch  16 |  1000/ 2983 batches | lr 1.25 | ms/batch 22.32 | loss  4.19 | ppl    65.79\n",
      "| epoch  16 |  1200/ 2983 batches | lr 1.25 | ms/batch 22.27 | loss  4.19 | ppl    66.15\n",
      "| epoch  16 |  1400/ 2983 batches | lr 1.25 | ms/batch 22.35 | loss  4.22 | ppl    68.20\n",
      "| epoch  16 |  1600/ 2983 batches | lr 1.25 | ms/batch 22.27 | loss  4.28 | ppl    72.20\n",
      "| epoch  16 |  1800/ 2983 batches | lr 1.25 | ms/batch 22.35 | loss  4.16 | ppl    64.27\n",
      "| epoch  16 |  2000/ 2983 batches | lr 1.25 | ms/batch 22.32 | loss  4.20 | ppl    66.83\n",
      "| epoch  16 |  2200/ 2983 batches | lr 1.25 | ms/batch 22.28 | loss  4.08 | ppl    59.00\n",
      "| epoch  16 |  2400/ 2983 batches | lr 1.25 | ms/batch 22.25 | loss  4.12 | ppl    61.33\n",
      "| epoch  16 |  2600/ 2983 batches | lr 1.25 | ms/batch 22.29 | loss  4.14 | ppl    62.54\n",
      "| epoch  16 |  2800/ 2983 batches | lr 1.25 | ms/batch 22.32 | loss  4.07 | ppl    58.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 69.29s | valid loss  4.70 | valid ppl   110.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 2983 batches | lr 0.31 | ms/batch 22.32 | loss  4.29 | ppl    72.75\n",
      "| epoch  17 |   400/ 2983 batches | lr 0.31 | ms/batch 22.28 | loss  4.31 | ppl    74.56\n",
      "| epoch  17 |   600/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.13 | ppl    62.20\n",
      "| epoch  17 |   800/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.21 | ppl    67.14\n",
      "| epoch  17 |  1000/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.20 | ppl    66.97\n",
      "| epoch  17 |  1200/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.20 | ppl    66.86\n",
      "| epoch  17 |  1400/ 2983 batches | lr 0.31 | ms/batch 22.25 | loss  4.23 | ppl    68.50\n",
      "| epoch  17 |  1600/ 2983 batches | lr 0.31 | ms/batch 22.27 | loss  4.29 | ppl    72.69\n",
      "| epoch  17 |  1800/ 2983 batches | lr 0.31 | ms/batch 22.29 | loss  4.17 | ppl    64.76\n",
      "| epoch  17 |  2000/ 2983 batches | lr 0.31 | ms/batch 22.27 | loss  4.21 | ppl    67.17\n",
      "| epoch  17 |  2200/ 2983 batches | lr 0.31 | ms/batch 22.29 | loss  4.07 | ppl    58.74\n",
      "| epoch  17 |  2400/ 2983 batches | lr 0.31 | ms/batch 22.33 | loss  4.10 | ppl    60.43\n",
      "| epoch  17 |  2600/ 2983 batches | lr 0.31 | ms/batch 22.32 | loss  4.13 | ppl    62.42\n",
      "| epoch  17 |  2800/ 2983 batches | lr 0.31 | ms/batch 22.21 | loss  4.06 | ppl    58.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 69.18s | valid loss  4.69 | valid ppl   109.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 2983 batches | lr 0.31 | ms/batch 22.30 | loss  4.28 | ppl    72.16\n",
      "| epoch  18 |   400/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.30 | ppl    73.61\n",
      "| epoch  18 |   600/ 2983 batches | lr 0.31 | ms/batch 22.29 | loss  4.12 | ppl    61.56\n",
      "| epoch  18 |   800/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.19 | ppl    66.20\n",
      "| epoch  18 |  1000/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.19 | ppl    66.00\n",
      "| epoch  18 |  1200/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.19 | ppl    66.09\n",
      "| epoch  18 |  1400/ 2983 batches | lr 0.31 | ms/batch 22.28 | loss  4.22 | ppl    67.97\n",
      "| epoch  18 |  1600/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.28 | ppl    72.14\n",
      "| epoch  18 |  1800/ 2983 batches | lr 0.31 | ms/batch 22.28 | loss  4.16 | ppl    64.38\n",
      "| epoch  18 |  2000/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.20 | ppl    66.62\n",
      "| epoch  18 |  2200/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.08 | ppl    58.89\n",
      "| epoch  18 |  2400/ 2983 batches | lr 0.31 | ms/batch 22.22 | loss  4.10 | ppl    60.54\n",
      "| epoch  18 |  2600/ 2983 batches | lr 0.31 | ms/batch 22.22 | loss  4.13 | ppl    62.48\n",
      "| epoch  18 |  2800/ 2983 batches | lr 0.31 | ms/batch 22.29 | loss  4.07 | ppl    58.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 69.26s | valid loss  4.69 | valid ppl   109.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 2983 batches | lr 0.31 | ms/batch 22.30 | loss  4.27 | ppl    71.59\n",
      "| epoch  19 |   400/ 2983 batches | lr 0.31 | ms/batch 22.25 | loss  4.29 | ppl    72.66\n",
      "| epoch  19 |   600/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.11 | ppl    60.83\n",
      "| epoch  19 |   800/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.18 | ppl    65.64\n",
      "| epoch  19 |  1000/ 2983 batches | lr 0.31 | ms/batch 22.22 | loss  4.18 | ppl    65.26\n",
      "| epoch  19 |  1200/ 2983 batches | lr 0.31 | ms/batch 22.25 | loss  4.19 | ppl    65.83\n",
      "| epoch  19 |  1400/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.21 | ppl    67.66\n",
      "| epoch  19 |  1600/ 2983 batches | lr 0.31 | ms/batch 22.25 | loss  4.28 | ppl    71.90\n",
      "| epoch  19 |  1800/ 2983 batches | lr 0.31 | ms/batch 22.20 | loss  4.16 | ppl    64.34\n",
      "| epoch  19 |  2000/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.20 | ppl    66.55\n",
      "| epoch  19 |  2200/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.07 | ppl    58.47\n",
      "| epoch  19 |  2400/ 2983 batches | lr 0.31 | ms/batch 22.28 | loss  4.11 | ppl    60.67\n",
      "| epoch  19 |  2600/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.13 | ppl    62.21\n",
      "| epoch  19 |  2800/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.07 | ppl    58.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 69.13s | valid loss  4.69 | valid ppl   108.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 2983 batches | lr 0.31 | ms/batch 22.37 | loss  4.27 | ppl    71.43\n",
      "| epoch  20 |   400/ 2983 batches | lr 0.31 | ms/batch 22.29 | loss  4.28 | ppl    72.59\n",
      "| epoch  20 |   600/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.10 | ppl    60.53\n",
      "| epoch  20 |   800/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.18 | ppl    65.68\n",
      "| epoch  20 |  1000/ 2983 batches | lr 0.31 | ms/batch 22.27 | loss  4.18 | ppl    65.06\n",
      "| epoch  20 |  1200/ 2983 batches | lr 0.31 | ms/batch 22.20 | loss  4.18 | ppl    65.14\n",
      "| epoch  20 |  1400/ 2983 batches | lr 0.31 | ms/batch 22.25 | loss  4.21 | ppl    67.56\n",
      "| epoch  20 |  1600/ 2983 batches | lr 0.31 | ms/batch 22.23 | loss  4.27 | ppl    71.42\n",
      "| epoch  20 |  1800/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.16 | ppl    63.88\n",
      "| epoch  20 |  2000/ 2983 batches | lr 0.31 | ms/batch 22.34 | loss  4.20 | ppl    66.45\n",
      "| epoch  20 |  2200/ 2983 batches | lr 0.31 | ms/batch 22.32 | loss  4.07 | ppl    58.51\n",
      "| epoch  20 |  2400/ 2983 batches | lr 0.31 | ms/batch 22.20 | loss  4.10 | ppl    60.49\n",
      "| epoch  20 |  2600/ 2983 batches | lr 0.31 | ms/batch 22.20 | loss  4.13 | ppl    62.41\n",
      "| epoch  20 |  2800/ 2983 batches | lr 0.31 | ms/batch 22.20 | loss  4.06 | ppl    58.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 69.14s | valid loss  4.69 | valid ppl   108.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/ 2983 batches | lr 0.31 | ms/batch 22.29 | loss  4.26 | ppl    71.06\n",
      "| epoch  21 |   400/ 2983 batches | lr 0.31 | ms/batch 22.23 | loss  4.28 | ppl    72.02\n",
      "| epoch  21 |   600/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.10 | ppl    60.36\n",
      "| epoch  21 |   800/ 2983 batches | lr 0.31 | ms/batch 22.35 | loss  4.17 | ppl    64.95\n",
      "| epoch  21 |  1000/ 2983 batches | lr 0.31 | ms/batch 22.32 | loss  4.17 | ppl    64.72\n",
      "| epoch  21 |  1200/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.18 | ppl    65.22\n",
      "| epoch  21 |  1400/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.21 | ppl    67.26\n",
      "| epoch  21 |  1600/ 2983 batches | lr 0.31 | ms/batch 22.24 | loss  4.27 | ppl    71.32\n",
      "| epoch  21 |  1800/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.16 | ppl    63.96\n",
      "| epoch  21 |  2000/ 2983 batches | lr 0.31 | ms/batch 22.25 | loss  4.19 | ppl    66.29\n",
      "| epoch  21 |  2200/ 2983 batches | lr 0.31 | ms/batch 22.19 | loss  4.07 | ppl    58.38\n",
      "| epoch  21 |  2400/ 2983 batches | lr 0.31 | ms/batch 22.27 | loss  4.10 | ppl    60.20\n",
      "| epoch  21 |  2600/ 2983 batches | lr 0.31 | ms/batch 22.26 | loss  4.13 | ppl    62.15\n",
      "| epoch  21 |  2800/ 2983 batches | lr 0.31 | ms/batch 22.21 | loss  4.06 | ppl    58.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 69.14s | valid loss  4.69 | valid ppl   108.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/ 2983 batches | lr 0.08 | ms/batch 22.37 | loss  4.27 | ppl    71.38\n",
      "| epoch  22 |   400/ 2983 batches | lr 0.08 | ms/batch 22.30 | loss  4.29 | ppl    73.14\n",
      "| epoch  22 |   600/ 2983 batches | lr 0.08 | ms/batch 22.27 | loss  4.11 | ppl    60.70\n",
      "| epoch  22 |   800/ 2983 batches | lr 0.08 | ms/batch 22.28 | loss  4.19 | ppl    65.73\n",
      "| epoch  22 |  1000/ 2983 batches | lr 0.08 | ms/batch 22.28 | loss  4.18 | ppl    65.26\n",
      "| epoch  22 |  1200/ 2983 batches | lr 0.08 | ms/batch 22.31 | loss  4.18 | ppl    65.54\n",
      "| epoch  22 |  1400/ 2983 batches | lr 0.08 | ms/batch 22.39 | loss  4.21 | ppl    67.37\n",
      "| epoch  22 |  1600/ 2983 batches | lr 0.08 | ms/batch 22.29 | loss  4.27 | ppl    71.79\n",
      "| epoch  22 |  1800/ 2983 batches | lr 0.08 | ms/batch 22.30 | loss  4.16 | ppl    63.95\n",
      "| epoch  22 |  2000/ 2983 batches | lr 0.08 | ms/batch 22.29 | loss  4.20 | ppl    66.43\n",
      "| epoch  22 |  2200/ 2983 batches | lr 0.08 | ms/batch 22.28 | loss  4.07 | ppl    58.70\n",
      "| epoch  22 |  2400/ 2983 batches | lr 0.08 | ms/batch 22.31 | loss  4.09 | ppl    59.86\n",
      "| epoch  22 |  2600/ 2983 batches | lr 0.08 | ms/batch 22.32 | loss  4.12 | ppl    61.62\n",
      "| epoch  22 |  2800/ 2983 batches | lr 0.08 | ms/batch 22.31 | loss  4.06 | ppl    57.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 69.28s | valid loss  4.68 | valid ppl   108.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/ 2983 batches | lr 0.08 | ms/batch 22.34 | loss  4.27 | ppl    71.30\n",
      "| epoch  23 |   400/ 2983 batches | lr 0.08 | ms/batch 22.19 | loss  4.28 | ppl    72.41\n",
      "| epoch  23 |   600/ 2983 batches | lr 0.08 | ms/batch 22.27 | loss  4.10 | ppl    60.62\n",
      "| epoch  23 |   800/ 2983 batches | lr 0.08 | ms/batch 22.27 | loss  4.18 | ppl    65.28\n",
      "| epoch  23 |  1000/ 2983 batches | lr 0.08 | ms/batch 22.28 | loss  4.17 | ppl    64.73\n",
      "| epoch  23 |  1200/ 2983 batches | lr 0.08 | ms/batch 22.28 | loss  4.18 | ppl    65.12\n",
      "| epoch  23 |  1400/ 2983 batches | lr 0.08 | ms/batch 22.28 | loss  4.21 | ppl    67.35\n",
      "| epoch  23 |  1600/ 2983 batches | lr 0.08 | ms/batch 22.24 | loss  4.27 | ppl    71.52\n",
      "| epoch  23 |  1800/ 2983 batches | lr 0.08 | ms/batch 22.34 | loss  4.16 | ppl    63.85\n",
      "| epoch  23 |  2000/ 2983 batches | lr 0.08 | ms/batch 22.39 | loss  4.19 | ppl    65.82\n",
      "| epoch  23 |  2200/ 2983 batches | lr 0.08 | ms/batch 22.23 | loss  4.07 | ppl    58.42\n",
      "| epoch  23 |  2400/ 2983 batches | lr 0.08 | ms/batch 22.27 | loss  4.09 | ppl    59.87\n",
      "| epoch  23 |  2600/ 2983 batches | lr 0.08 | ms/batch 22.26 | loss  4.12 | ppl    61.79\n",
      "| epoch  23 |  2800/ 2983 batches | lr 0.08 | ms/batch 22.21 | loss  4.06 | ppl    57.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 69.18s | valid loss  4.68 | valid ppl   108.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/ 2983 batches | lr 0.08 | ms/batch 22.38 | loss  4.26 | ppl    70.60\n",
      "| epoch  24 |   400/ 2983 batches | lr 0.08 | ms/batch 22.24 | loss  4.28 | ppl    72.17\n",
      "| epoch  24 |   600/ 2983 batches | lr 0.08 | ms/batch 22.26 | loss  4.11 | ppl    60.65\n",
      "| epoch  24 |   800/ 2983 batches | lr 0.08 | ms/batch 22.25 | loss  4.18 | ppl    65.30\n",
      "| epoch  24 |  1000/ 2983 batches | lr 0.08 | ms/batch 22.32 | loss  4.17 | ppl    64.60\n",
      "| epoch  24 |  1200/ 2983 batches | lr 0.08 | ms/batch 22.30 | loss  4.18 | ppl    65.29\n",
      "| epoch  24 |  1400/ 2983 batches | lr 0.08 | ms/batch 22.25 | loss  4.21 | ppl    67.08\n",
      "| epoch  24 |  1600/ 2983 batches | lr 0.08 | ms/batch 22.29 | loss  4.27 | ppl    71.57\n",
      "| epoch  24 |  1800/ 2983 batches | lr 0.08 | ms/batch 22.26 | loss  4.16 | ppl    63.77\n",
      "| epoch  24 |  2000/ 2983 batches | lr 0.08 | ms/batch 22.26 | loss  4.19 | ppl    66.17\n",
      "| epoch  24 |  2200/ 2983 batches | lr 0.08 | ms/batch 22.28 | loss  4.07 | ppl    58.41\n",
      "| epoch  24 |  2400/ 2983 batches | lr 0.08 | ms/batch 22.37 | loss  4.09 | ppl    59.66\n",
      "| epoch  24 |  2600/ 2983 batches | lr 0.08 | ms/batch 22.31 | loss  4.13 | ppl    62.12\n",
      "| epoch  24 |  2800/ 2983 batches | lr 0.08 | ms/batch 22.29 | loss  4.06 | ppl    57.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 69.23s | valid loss  4.68 | valid ppl   108.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/ 2983 batches | lr 0.02 | ms/batch 22.45 | loss  4.26 | ppl    71.15\n",
      "| epoch  25 |   400/ 2983 batches | lr 0.02 | ms/batch 22.37 | loss  4.28 | ppl    72.41\n",
      "| epoch  25 |   600/ 2983 batches | lr 0.02 | ms/batch 22.34 | loss  4.10 | ppl    60.48\n",
      "| epoch  25 |   800/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.19 | ppl    65.73\n",
      "| epoch  25 |  1000/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.18 | ppl    65.12\n",
      "| epoch  25 |  1200/ 2983 batches | lr 0.02 | ms/batch 22.25 | loss  4.18 | ppl    65.24\n",
      "| epoch  25 |  1400/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.21 | ppl    67.38\n",
      "| epoch  25 |  1600/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.27 | ppl    71.32\n",
      "| epoch  25 |  1800/ 2983 batches | lr 0.02 | ms/batch 22.26 | loss  4.16 | ppl    63.84\n",
      "| epoch  25 |  2000/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.19 | ppl    66.08\n",
      "| epoch  25 |  2200/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.07 | ppl    58.62\n",
      "| epoch  25 |  2400/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.09 | ppl    59.91\n",
      "| epoch  25 |  2600/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.12 | ppl    61.67\n",
      "| epoch  25 |  2800/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.06 | ppl    57.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 69.31s | valid loss  4.68 | valid ppl   108.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/ 2983 batches | lr 0.02 | ms/batch 22.38 | loss  4.26 | ppl    70.62\n",
      "| epoch  26 |   400/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.28 | ppl    72.40\n",
      "| epoch  26 |   600/ 2983 batches | lr 0.02 | ms/batch 22.24 | loss  4.10 | ppl    60.55\n",
      "| epoch  26 |   800/ 2983 batches | lr 0.02 | ms/batch 22.24 | loss  4.19 | ppl    65.78\n",
      "| epoch  26 |  1000/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.18 | ppl    65.06\n",
      "| epoch  26 |  1200/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.18 | ppl    65.10\n",
      "| epoch  26 |  1400/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.21 | ppl    67.28\n",
      "| epoch  26 |  1600/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.27 | ppl    71.59\n",
      "| epoch  26 |  1800/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.15 | ppl    63.74\n",
      "| epoch  26 |  2000/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.19 | ppl    65.84\n",
      "| epoch  26 |  2200/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.07 | ppl    58.55\n",
      "| epoch  26 |  2400/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.09 | ppl    59.63\n",
      "| epoch  26 |  2600/ 2983 batches | lr 0.02 | ms/batch 22.21 | loss  4.12 | ppl    61.85\n",
      "| epoch  26 |  2800/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.06 | ppl    57.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 69.26s | valid loss  4.68 | valid ppl   107.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/ 2983 batches | lr 0.02 | ms/batch 22.44 | loss  4.26 | ppl    70.63\n",
      "| epoch  27 |   400/ 2983 batches | lr 0.02 | ms/batch 22.32 | loss  4.28 | ppl    72.41\n",
      "| epoch  27 |   600/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.10 | ppl    60.30\n",
      "| epoch  27 |   800/ 2983 batches | lr 0.02 | ms/batch 22.32 | loss  4.18 | ppl    65.63\n",
      "| epoch  27 |  1000/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.17 | ppl    64.70\n",
      "| epoch  27 |  1200/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.18 | ppl    65.16\n",
      "| epoch  27 |  1400/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.21 | ppl    67.40\n",
      "| epoch  27 |  1600/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.27 | ppl    71.27\n",
      "| epoch  27 |  1800/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.16 | ppl    63.93\n",
      "| epoch  27 |  2000/ 2983 batches | lr 0.02 | ms/batch 22.32 | loss  4.19 | ppl    66.25\n",
      "| epoch  27 |  2200/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.07 | ppl    58.65\n",
      "| epoch  27 |  2400/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.09 | ppl    59.51\n",
      "| epoch  27 |  2600/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.13 | ppl    61.97\n",
      "| epoch  27 |  2800/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.06 | ppl    57.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 69.30s | valid loss  4.68 | valid ppl   107.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/ 2983 batches | lr 0.02 | ms/batch 22.39 | loss  4.25 | ppl    70.42\n",
      "| epoch  28 |   400/ 2983 batches | lr 0.02 | ms/batch 22.35 | loss  4.28 | ppl    72.12\n",
      "| epoch  28 |   600/ 2983 batches | lr 0.02 | ms/batch 22.35 | loss  4.10 | ppl    60.30\n",
      "| epoch  28 |   800/ 2983 batches | lr 0.02 | ms/batch 22.34 | loss  4.18 | ppl    65.38\n",
      "| epoch  28 |  1000/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.17 | ppl    64.92\n",
      "| epoch  28 |  1200/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.17 | ppl    65.03\n",
      "| epoch  28 |  1400/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.21 | ppl    67.27\n",
      "| epoch  28 |  1600/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.27 | ppl    71.57\n",
      "| epoch  28 |  1800/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.15 | ppl    63.67\n",
      "| epoch  28 |  2000/ 2983 batches | lr 0.02 | ms/batch 22.22 | loss  4.18 | ppl    65.57\n",
      "| epoch  28 |  2200/ 2983 batches | lr 0.02 | ms/batch 22.35 | loss  4.07 | ppl    58.51\n",
      "| epoch  28 |  2400/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.09 | ppl    59.63\n",
      "| epoch  28 |  2600/ 2983 batches | lr 0.02 | ms/batch 22.22 | loss  4.13 | ppl    61.97\n",
      "| epoch  28 |  2800/ 2983 batches | lr 0.02 | ms/batch 22.25 | loss  4.06 | ppl    58.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 69.26s | valid loss  4.68 | valid ppl   107.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/ 2983 batches | lr 0.02 | ms/batch 22.40 | loss  4.26 | ppl    70.65\n",
      "| epoch  29 |   400/ 2983 batches | lr 0.02 | ms/batch 22.25 | loss  4.28 | ppl    72.04\n",
      "| epoch  29 |   600/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.10 | ppl    60.60\n",
      "| epoch  29 |   800/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.18 | ppl    65.36\n",
      "| epoch  29 |  1000/ 2983 batches | lr 0.02 | ms/batch 22.33 | loss  4.17 | ppl    64.86\n",
      "| epoch  29 |  1200/ 2983 batches | lr 0.02 | ms/batch 22.37 | loss  4.18 | ppl    65.30\n",
      "| epoch  29 |  1400/ 2983 batches | lr 0.02 | ms/batch 22.33 | loss  4.21 | ppl    67.37\n",
      "| epoch  29 |  1600/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.27 | ppl    71.40\n",
      "| epoch  29 |  1800/ 2983 batches | lr 0.02 | ms/batch 22.37 | loss  4.16 | ppl    63.98\n",
      "| epoch  29 |  2000/ 2983 batches | lr 0.02 | ms/batch 22.37 | loss  4.19 | ppl    66.09\n",
      "| epoch  29 |  2200/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.07 | ppl    58.51\n",
      "| epoch  29 |  2400/ 2983 batches | lr 0.02 | ms/batch 22.23 | loss  4.09 | ppl    59.53\n",
      "| epoch  29 |  2600/ 2983 batches | lr 0.02 | ms/batch 22.24 | loss  4.13 | ppl    62.01\n",
      "| epoch  29 |  2800/ 2983 batches | lr 0.02 | ms/batch 22.25 | loss  4.06 | ppl    57.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 69.29s | valid loss  4.68 | valid ppl   107.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/ 2983 batches | lr 0.02 | ms/batch 22.33 | loss  4.26 | ppl    70.57\n",
      "| epoch  30 |   400/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.28 | ppl    72.29\n",
      "| epoch  30 |   600/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.10 | ppl    60.14\n",
      "| epoch  30 |   800/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.18 | ppl    65.29\n",
      "| epoch  30 |  1000/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.17 | ppl    64.47\n",
      "| epoch  30 |  1200/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.18 | ppl    65.13\n",
      "| epoch  30 |  1400/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.21 | ppl    67.18\n",
      "| epoch  30 |  1600/ 2983 batches | lr 0.02 | ms/batch 22.32 | loss  4.27 | ppl    71.36\n",
      "| epoch  30 |  1800/ 2983 batches | lr 0.02 | ms/batch 22.31 | loss  4.15 | ppl    63.64\n",
      "| epoch  30 |  2000/ 2983 batches | lr 0.02 | ms/batch 22.34 | loss  4.19 | ppl    66.05\n",
      "| epoch  30 |  2200/ 2983 batches | lr 0.02 | ms/batch 22.32 | loss  4.07 | ppl    58.52\n",
      "| epoch  30 |  2400/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.09 | ppl    59.55\n",
      "| epoch  30 |  2600/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.13 | ppl    62.17\n",
      "| epoch  30 |  2800/ 2983 batches | lr 0.02 | ms/batch 22.26 | loss  4.06 | ppl    58.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 69.26s | valid loss  4.68 | valid ppl   107.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   200/ 2983 batches | lr 0.02 | ms/batch 22.40 | loss  4.26 | ppl    70.50\n",
      "| epoch  31 |   400/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.27 | ppl    71.88\n",
      "| epoch  31 |   600/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.10 | ppl    60.34\n",
      "| epoch  31 |   800/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.18 | ppl    65.40\n",
      "| epoch  31 |  1000/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.17 | ppl    64.72\n",
      "| epoch  31 |  1200/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.17 | ppl    64.82\n",
      "| epoch  31 |  1400/ 2983 batches | lr 0.02 | ms/batch 22.32 | loss  4.21 | ppl    67.14\n",
      "| epoch  31 |  1600/ 2983 batches | lr 0.02 | ms/batch 22.22 | loss  4.27 | ppl    71.60\n",
      "| epoch  31 |  1800/ 2983 batches | lr 0.02 | ms/batch 22.22 | loss  4.15 | ppl    63.68\n",
      "| epoch  31 |  2000/ 2983 batches | lr 0.02 | ms/batch 22.33 | loss  4.19 | ppl    65.97\n",
      "| epoch  31 |  2200/ 2983 batches | lr 0.02 | ms/batch 22.34 | loss  4.07 | ppl    58.40\n",
      "| epoch  31 |  2400/ 2983 batches | lr 0.02 | ms/batch 22.32 | loss  4.09 | ppl    59.64\n",
      "| epoch  31 |  2600/ 2983 batches | lr 0.02 | ms/batch 22.20 | loss  4.12 | ppl    61.81\n",
      "| epoch  31 |  2800/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.06 | ppl    57.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 69.23s | valid loss  4.68 | valid ppl   107.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   200/ 2983 batches | lr 0.02 | ms/batch 22.40 | loss  4.25 | ppl    70.38\n",
      "| epoch  32 |   400/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.28 | ppl    72.12\n",
      "| epoch  32 |   600/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.10 | ppl    60.30\n",
      "| epoch  32 |   800/ 2983 batches | lr 0.02 | ms/batch 22.27 | loss  4.18 | ppl    65.33\n",
      "| epoch  32 |  1000/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.17 | ppl    64.83\n",
      "| epoch  32 |  1200/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.18 | ppl    65.16\n",
      "| epoch  32 |  1400/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.21 | ppl    67.30\n",
      "| epoch  32 |  1600/ 2983 batches | lr 0.02 | ms/batch 22.30 | loss  4.27 | ppl    71.51\n",
      "| epoch  32 |  1800/ 2983 batches | lr 0.02 | ms/batch 22.28 | loss  4.16 | ppl    63.77\n",
      "| epoch  32 |  2000/ 2983 batches | lr 0.02 | ms/batch 22.29 | loss  4.19 | ppl    65.73\n",
      "| epoch  32 |  2200/ 2983 batches | lr 0.02 | ms/batch 22.23 | loss  4.07 | ppl    58.33\n",
      "| epoch  32 |  2400/ 2983 batches | lr 0.02 | ms/batch 22.32 | loss  4.09 | ppl    59.54\n",
      "| epoch  32 |  2600/ 2983 batches | lr 0.02 | ms/batch 22.34 | loss  4.13 | ppl    62.03\n",
      "| epoch  32 |  2800/ 2983 batches | lr 0.02 | ms/batch 22.34 | loss  4.06 | ppl    57.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 69.28s | valid loss  4.68 | valid ppl   107.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   200/ 2983 batches | lr 0.00 | ms/batch 22.38 | loss  4.26 | ppl    70.49\n",
      "| epoch  33 |   400/ 2983 batches | lr 0.00 | ms/batch 22.33 | loss  4.28 | ppl    72.19\n",
      "| epoch  33 |   600/ 2983 batches | lr 0.00 | ms/batch 22.31 | loss  4.10 | ppl    60.16\n",
      "| epoch  33 |   800/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.17 | ppl    64.89\n",
      "| epoch  33 |  1000/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.17 | ppl    64.98\n",
      "| epoch  33 |  1200/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.17 | ppl    64.95\n",
      "| epoch  33 |  1400/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.21 | ppl    67.17\n",
      "| epoch  33 |  1600/ 2983 batches | lr 0.00 | ms/batch 22.21 | loss  4.26 | ppl    71.16\n",
      "| epoch  33 |  1800/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.16 | ppl    63.84\n",
      "| epoch  33 |  2000/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.19 | ppl    65.93\n",
      "| epoch  33 |  2200/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.07 | ppl    58.39\n",
      "| epoch  33 |  2400/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.09 | ppl    59.64\n",
      "| epoch  33 |  2600/ 2983 batches | lr 0.00 | ms/batch 22.32 | loss  4.12 | ppl    61.78\n",
      "| epoch  33 |  2800/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.06 | ppl    57.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 69.37s | valid loss  4.68 | valid ppl   107.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   200/ 2983 batches | lr 0.00 | ms/batch 22.48 | loss  4.26 | ppl    70.56\n",
      "| epoch  34 |   400/ 2983 batches | lr 0.00 | ms/batch 22.37 | loss  4.28 | ppl    72.17\n",
      "| epoch  34 |   600/ 2983 batches | lr 0.00 | ms/batch 22.33 | loss  4.10 | ppl    60.32\n",
      "| epoch  34 |   800/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.18 | ppl    65.08\n",
      "| epoch  34 |  1000/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.17 | ppl    64.79\n",
      "| epoch  34 |  1200/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.18 | ppl    65.26\n",
      "| epoch  34 |  1400/ 2983 batches | lr 0.00 | ms/batch 22.31 | loss  4.21 | ppl    67.18\n",
      "| epoch  34 |  1600/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.27 | ppl    71.19\n",
      "| epoch  34 |  1800/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.16 | ppl    63.92\n",
      "| epoch  34 |  2000/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.19 | ppl    66.05\n",
      "| epoch  34 |  2200/ 2983 batches | lr 0.00 | ms/batch 22.23 | loss  4.07 | ppl    58.48\n",
      "| epoch  34 |  2400/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.09 | ppl    59.75\n",
      "| epoch  34 |  2600/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.13 | ppl    61.93\n",
      "| epoch  34 |  2800/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.06 | ppl    57.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 69.27s | valid loss  4.68 | valid ppl   107.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   200/ 2983 batches | lr 0.00 | ms/batch 22.35 | loss  4.26 | ppl    70.83\n",
      "| epoch  35 |   400/ 2983 batches | lr 0.00 | ms/batch 22.32 | loss  4.28 | ppl    72.29\n",
      "| epoch  35 |   600/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.10 | ppl    60.19\n",
      "| epoch  35 |   800/ 2983 batches | lr 0.00 | ms/batch 22.35 | loss  4.17 | ppl    65.00\n",
      "| epoch  35 |  1000/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.17 | ppl    65.01\n",
      "| epoch  35 |  1200/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.18 | ppl    65.16\n",
      "| epoch  35 |  1400/ 2983 batches | lr 0.00 | ms/batch 22.20 | loss  4.21 | ppl    67.11\n",
      "| epoch  35 |  1600/ 2983 batches | lr 0.00 | ms/batch 22.22 | loss  4.26 | ppl    70.94\n",
      "| epoch  35 |  1800/ 2983 batches | lr 0.00 | ms/batch 22.23 | loss  4.15 | ppl    63.69\n",
      "| epoch  35 |  2000/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.19 | ppl    65.73\n",
      "| epoch  35 |  2200/ 2983 batches | lr 0.00 | ms/batch 22.24 | loss  4.07 | ppl    58.68\n",
      "| epoch  35 |  2400/ 2983 batches | lr 0.00 | ms/batch 22.25 | loss  4.09 | ppl    59.71\n",
      "| epoch  35 |  2600/ 2983 batches | lr 0.00 | ms/batch 22.25 | loss  4.13 | ppl    61.91\n",
      "| epoch  35 |  2800/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.06 | ppl    57.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 69.19s | valid loss  4.68 | valid ppl   107.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   200/ 2983 batches | lr 0.00 | ms/batch 22.39 | loss  4.25 | ppl    70.25\n",
      "| epoch  36 |   400/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.28 | ppl    72.23\n",
      "| epoch  36 |   600/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.10 | ppl    60.27\n",
      "| epoch  36 |   800/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.18 | ppl    65.19\n",
      "| epoch  36 |  1000/ 2983 batches | lr 0.00 | ms/batch 22.31 | loss  4.17 | ppl    64.61\n",
      "| epoch  36 |  1200/ 2983 batches | lr 0.00 | ms/batch 22.39 | loss  4.17 | ppl    64.94\n",
      "| epoch  36 |  1400/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.21 | ppl    67.09\n",
      "| epoch  36 |  1600/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.26 | ppl    71.16\n",
      "| epoch  36 |  1800/ 2983 batches | lr 0.00 | ms/batch 22.25 | loss  4.16 | ppl    63.84\n",
      "| epoch  36 |  2000/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.19 | ppl    65.92\n",
      "| epoch  36 |  2200/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.06 | ppl    58.25\n",
      "| epoch  36 |  2400/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.09 | ppl    59.65\n",
      "| epoch  36 |  2600/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.12 | ppl    61.84\n",
      "| epoch  36 |  2800/ 2983 batches | lr 0.00 | ms/batch 22.31 | loss  4.06 | ppl    57.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 69.27s | valid loss  4.68 | valid ppl   107.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   200/ 2983 batches | lr 0.00 | ms/batch 22.35 | loss  4.26 | ppl    71.06\n",
      "| epoch  37 |   400/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.28 | ppl    72.19\n",
      "| epoch  37 |   600/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.10 | ppl    60.24\n",
      "| epoch  37 |   800/ 2983 batches | lr 0.00 | ms/batch 22.23 | loss  4.18 | ppl    65.25\n",
      "| epoch  37 |  1000/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.17 | ppl    64.70\n",
      "| epoch  37 |  1200/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.18 | ppl    65.14\n",
      "| epoch  37 |  1400/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.21 | ppl    67.20\n",
      "| epoch  37 |  1600/ 2983 batches | lr 0.00 | ms/batch 22.34 | loss  4.27 | ppl    71.27\n",
      "| epoch  37 |  1800/ 2983 batches | lr 0.00 | ms/batch 22.33 | loss  4.16 | ppl    63.79\n",
      "| epoch  37 |  2000/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.18 | ppl    65.66\n",
      "| epoch  37 |  2200/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.07 | ppl    58.46\n",
      "| epoch  37 |  2400/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.09 | ppl    59.50\n",
      "| epoch  37 |  2600/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.13 | ppl    62.07\n",
      "| epoch  37 |  2800/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.06 | ppl    57.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 69.23s | valid loss  4.68 | valid ppl   107.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   200/ 2983 batches | lr 0.00 | ms/batch 22.40 | loss  4.26 | ppl    70.59\n",
      "| epoch  38 |   400/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.28 | ppl    72.21\n",
      "| epoch  38 |   600/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.10 | ppl    60.33\n",
      "| epoch  38 |   800/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.18 | ppl    65.15\n",
      "| epoch  38 |  1000/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.17 | ppl    64.81\n",
      "| epoch  38 |  1200/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.18 | ppl    65.30\n",
      "| epoch  38 |  1400/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.21 | ppl    67.23\n",
      "| epoch  38 |  1600/ 2983 batches | lr 0.00 | ms/batch 22.35 | loss  4.27 | ppl    71.25\n",
      "| epoch  38 |  1800/ 2983 batches | lr 0.00 | ms/batch 22.34 | loss  4.16 | ppl    63.80\n",
      "| epoch  38 |  2000/ 2983 batches | lr 0.00 | ms/batch 22.34 | loss  4.19 | ppl    65.97\n",
      "| epoch  38 |  2200/ 2983 batches | lr 0.00 | ms/batch 22.31 | loss  4.07 | ppl    58.29\n",
      "| epoch  38 |  2400/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.09 | ppl    59.50\n",
      "| epoch  38 |  2600/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.13 | ppl    62.03\n",
      "| epoch  38 |  2800/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.06 | ppl    57.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 69.27s | valid loss  4.68 | valid ppl   107.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   200/ 2983 batches | lr 0.00 | ms/batch 22.41 | loss  4.25 | ppl    70.36\n",
      "| epoch  39 |   400/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.28 | ppl    72.11\n",
      "| epoch  39 |   600/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.10 | ppl    60.26\n",
      "| epoch  39 |   800/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.18 | ppl    65.07\n",
      "| epoch  39 |  1000/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.17 | ppl    64.84\n",
      "| epoch  39 |  1200/ 2983 batches | lr 0.00 | ms/batch 22.32 | loss  4.18 | ppl    65.07\n",
      "| epoch  39 |  1400/ 2983 batches | lr 0.00 | ms/batch 22.28 | loss  4.21 | ppl    67.22\n",
      "| epoch  39 |  1600/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.27 | ppl    71.33\n",
      "| epoch  39 |  1800/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.15 | ppl    63.73\n",
      "| epoch  39 |  2000/ 2983 batches | lr 0.00 | ms/batch 22.26 | loss  4.19 | ppl    65.73\n",
      "| epoch  39 |  2200/ 2983 batches | lr 0.00 | ms/batch 22.33 | loss  4.07 | ppl    58.32\n",
      "| epoch  39 |  2400/ 2983 batches | lr 0.00 | ms/batch 22.25 | loss  4.09 | ppl    59.57\n",
      "| epoch  39 |  2600/ 2983 batches | lr 0.00 | ms/batch 22.33 | loss  4.12 | ppl    61.86\n",
      "| epoch  39 |  2800/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.06 | ppl    57.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 69.26s | valid loss  4.68 | valid ppl   107.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   200/ 2983 batches | lr 0.00 | ms/batch 22.37 | loss  4.25 | ppl    70.44\n",
      "| epoch  40 |   400/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.28 | ppl    72.12\n",
      "| epoch  40 |   600/ 2983 batches | lr 0.00 | ms/batch 22.29 | loss  4.10 | ppl    60.17\n",
      "| epoch  40 |   800/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.18 | ppl    65.30\n",
      "| epoch  40 |  1000/ 2983 batches | lr 0.00 | ms/batch 22.32 | loss  4.17 | ppl    64.88\n",
      "| epoch  40 |  1200/ 2983 batches | lr 0.00 | ms/batch 22.30 | loss  4.17 | ppl    64.95\n",
      "| epoch  40 |  1400/ 2983 batches | lr 0.00 | ms/batch 22.24 | loss  4.21 | ppl    67.16\n",
      "| epoch  40 |  1600/ 2983 batches | lr 0.00 | ms/batch 22.25 | loss  4.27 | ppl    71.27\n",
      "| epoch  40 |  1800/ 2983 batches | lr 0.00 | ms/batch 22.22 | loss  4.15 | ppl    63.72\n",
      "| epoch  40 |  2000/ 2983 batches | lr 0.00 | ms/batch 22.32 | loss  4.19 | ppl    66.02\n",
      "| epoch  40 |  2200/ 2983 batches | lr 0.00 | ms/batch 22.25 | loss  4.07 | ppl    58.50\n",
      "| epoch  40 |  2400/ 2983 batches | lr 0.00 | ms/batch 22.27 | loss  4.09 | ppl    59.66\n",
      "| epoch  40 |  2600/ 2983 batches | lr 0.00 | ms/batch 22.31 | loss  4.12 | ppl    61.85\n",
      "| epoch  40 |  2800/ 2983 batches | lr 0.00 | ms/batch 22.25 | loss  4.06 | ppl    57.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 69.35s | valid loss  4.68 | valid ppl   107.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.74 | test ppl   114.96\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "lr = 20\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, 40+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(\"madel.pth\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(\"madel.pth\", 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9kreuqKunDA"
   },
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LanguageModel_From_Scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
