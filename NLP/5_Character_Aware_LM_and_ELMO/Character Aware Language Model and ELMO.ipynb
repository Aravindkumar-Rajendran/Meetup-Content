{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Character Aware Language Model](https://arxiv.org/pdf/1508.06615.pdf)\n",
    "\n",
    "We have character Aware Language Model Implemented here from scratch.\n",
    "Since the training is big we used validation data to train the model and test data as our validation data.\n",
    "\n",
    "Note- Since validation data is quite smaller that's why we see perplexity not improving much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/char_rnn.png)\n",
    "![](images/char_rnn_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "QNP_sB2AXynV",
    "outputId": "a2f0724d-c767-455f-fb28-e78878df427e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/data\n",
      "--2020-08-24 18:57:37--  https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5101618 (4.9M) [text/plain]\n",
      "Saving to: ‘ptb.train.txt’\n",
      "\n",
      "ptb.train.txt       100%[===================>]   4.87M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-08-24 18:57:37 (32.6 MB/s) - ‘ptb.train.txt’ saved [5101618/5101618]\n",
      "\n",
      "--2020-08-24 18:57:37--  https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 399782 (390K) [text/plain]\n",
      "Saving to: ‘ptb.valid.txt’\n",
      "\n",
      "ptb.valid.txt       100%[===================>] 390.41K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-08-24 18:57:38 (9.01 MB/s) - ‘ptb.valid.txt’ saved [399782/399782]\n",
      "\n",
      "--2020-08-24 18:57:38--  https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 449945 (439K) [text/plain]\n",
      "Saving to: ‘ptb.test.txt’\n",
      "\n",
      "ptb.test.txt        100%[===================>] 439.40K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-08-24 18:57:38 (12.6 MB/s) - ‘ptb.test.txt’ saved [449945/449945]\n",
      "\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "%cd data\n",
    "!wget https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\n",
    "!wget https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt\n",
    "!wget https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8AHZpWpZjXB"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "BPTT = 35\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "N_EPOCHS = 10\n",
    "INIT_LR = 0.5\n",
    "BATCH_SIZE = 32\n",
    "SCHEDULER_PATIENCE = 0\n",
    "SCHEDULER_FACTOR = 0.1\n",
    "SCHEDULER_THRESHOLD = 1e-1\n",
    "CLIP = 5.0\n",
    "\n",
    "EMBEDDING_DIM = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1J-ap_UamnQ"
   },
   "source": [
    "# Preparing data for LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hP-H6wTRYBbE"
   },
   "outputs": [],
   "source": [
    "def process_ptb(in_filename, out_filename):\n",
    "    with open(f'{in_filename}', 'r') as r:\n",
    "        data = r.read()\n",
    "        data = data.replace('\\n', '<eos>')\n",
    "        data = data.split()\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    for i, _ in enumerate(data[:-BPTT-1]):\n",
    "        examples.append({'words':data[i:i+BPTT], 'targets': data[i+1:i+BPTT+1]})\n",
    "\n",
    "    with open(f'{out_filename}', 'w') as w:\n",
    "        for example in examples:\n",
    "            json.dump(example, w)\n",
    "            w.write('\\n')\n",
    "\n",
    "process_ptb(os.path.join(DATA_DIR, 'ptb.test.txt'), os.path.join(DATA_DIR, 'ptb.test.jsonl'))\n",
    "process_ptb(os.path.join(DATA_DIR, 'ptb.valid.txt'), os.path.join(DATA_DIR, 'ptb.valid.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "pKWg8FU5YZUj",
    "outputId": "b155f805-5db0-42a0-f990-32f49ed67002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 characters in character vocab\n",
      "char vocab = ['<unk>', '<pad>', '<sos>', '<eos>', 'e', 't', 'o', 'a', 's', 'n', 'i', 'r', 'h', 'l', 'd', 'u', 'c', 'm', '<', '>', 'f', 'p', 'k', 'g', 'y', 'b', 'w', 'v', 'N', \"'\", '.', 'x', '$', 'j', '-', 'q', 'z', '&', '0', '1', '9', '3', '5', '#', '8', '2', '\\\\', '*', '4', '6', '7', '/']\n",
      "6023 words in target vocab\n",
      "most common words = [('the', 144224), ('<unk>', 121928), ('<eos>', 117894), ('N', 90966), ('of', 64120), ('to', 61193), ('a', 60789), ('and', 48656), ('in', 48641), (\"'s\", 30364)]\n"
     ]
    }
   ],
   "source": [
    "CHAR_NESTING = data.Field(batch_first=True, tokenize=list, init_token='<sos>', eos_token='<eos>')\n",
    "CHARS = data.NestedField(CHAR_NESTING)\n",
    "TARGETS = data.Field(batch_first=True)\n",
    "\n",
    "fields = {'words': ('chars', CHARS), 'targets': ('targets', TARGETS)}\n",
    "#get data from csv\n",
    "train, valid,  = data.TabularDataset.splits(\n",
    "                path = 'data',\n",
    "                train = 'ptb.valid.jsonl',\n",
    "                validation = 'ptb.test.jsonl',\n",
    "                format = 'json',\n",
    "                fields = fields\n",
    ")\n",
    "\n",
    "TARGETS.build_vocab(train)\n",
    "CHARS.build_vocab(train)\n",
    "\n",
    "print(f'{len(CHARS.vocab)} characters in character vocab')\n",
    "print(f'char vocab = {CHARS.vocab.itos}')\n",
    "\n",
    "print(f'{len(TARGETS.vocab)} words in target vocab')\n",
    "print(f'most common words = {TARGETS.vocab.freqs.most_common(10)}')\n",
    "\n",
    "train_iter, valid_iter, = data.Iterator.splits((train, valid,),\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             sort=False,\n",
    "                                             repeat=False,\n",
    "                                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "T_0MFVqCaR2e",
    "outputId": "aec72fa1-7fcc-40ba-e173-53247c386792"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.chars]:[torch.cuda.LongTensor of size 32x35x18 (GPU 0)]\n",
       "\t[.targets]:[torch.cuda.LongTensor of size 32x35 (GPU 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvwt_JVObZhJ"
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.mod =  modules\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # input shape [batch_size, seq_len, chars_vocab, emb_dim]\n",
    "        input_size = inputs.size()\n",
    "        output_shape = [-1]+[i for i in input_size[2:]]\n",
    "        # output_shape [batch_size*seq_len , chars_vocab, emb_dim]\n",
    "        inputs = inputs.view(*output_shape)\n",
    "        inputs = self.mod(inputs)\n",
    "        # inputs [batch_size*seq_len, chars_vocab, hid_dim]\n",
    "        output_shape = [input_size[0], input_size[1]] + [x for x in inputs.size()[1:]]\n",
    "        # output_shape [batch_size, seq_len, chars_vocab, hid_dim]\n",
    "        reshaped_input = inputs.view(*output_shape).contiguous()\n",
    "        return reshaped_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyLvfQdBbeBi"
   },
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,emb_dim,conv_layer,mul_factor,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.convModule = nn.ModuleList(\n",
    "            [TimeDistributed(nn.Conv1d(emb_dim, i*mul_factor , i)) for i in conv_layer]\n",
    "        )\n",
    "    def forward(self,input):\n",
    "        # input [batch_size, seq, nchars, emb]\n",
    "        convs = [ F.gelu(conv(input.transpose(2,3))) for conv in self.convModule]\n",
    "        # convs = [batch_size, seq, conv_layer[i]*25 , nchars-conv_layer[i]+1]\n",
    "        pool_out = [F.max_pool2d(conv, (1,conv.shape[3])).squeeze(-1) for conv in convs]\n",
    "        # pool_out [batch_size,seq,conv_layer[i]*25]\n",
    "        pool_cat = torch.cat(pool_out, dim = 2)\n",
    "        return pool_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1w3bnQiCbfgm"
   },
   "outputs": [],
   "source": [
    "class HighWayNetwork(nn.Module):\n",
    "    def __init__(self,in_dim):\n",
    "        super().__init__()\n",
    "        self.hx = nn.Linear(in_dim, in_dim)\n",
    "        self.tx = nn.Linear(in_dim, in_dim)\n",
    "    def forward(self,input):\n",
    "        #input [batch_size, seq_len, in_dim]\n",
    "        t = torch.sigmoid(self.tx(input))\n",
    "        # t [batch_size, seq_len, out_dim]\n",
    "        h = F.relu(self.hx(input))\n",
    "        # h [batch_size, seq_len, out_dim]\n",
    "        return h*t + input*(1-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfwzTcL6bhKo"
   },
   "outputs": [],
   "source": [
    "class CharLM(nn.Module):\n",
    "    def __init__(self, nchars, output_dim, emb_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        conv_layer = [2,3,4,5,6]\n",
    "        mul_factor = 20\n",
    "        self.emb = TimeDistributed(nn.Embedding(nchars, emb_dim,))\n",
    "        self.conv_layer = ConvLayer(emb_dim,conv_layer, mul_factor,dropout)\n",
    "        self.in_dim = sum(conv_layer)*mul_factor\n",
    "        self.highway_layer = TimeDistributed(HighWayNetwork(self.in_dim))\n",
    "        self.lstm = nn.LSTM(self.in_dim, self.in_dim//2 ,2,False,True,dropout, )\n",
    "        self.classify = nn.Linear(self.in_dim//2 ,output_dim,bias=False)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.emb(input)\n",
    "        conv_output = self.conv_layer(embed)\n",
    "        highway_output = self.highway_layer(conv_output)\n",
    "        output ,_ = self.lstm(highway_output , hidden)\n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        return self.classify(output) ,hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden =(\n",
    "            torch.zeros(2, batch_size, self.in_dim//2).to(device),\n",
    "            torch.zeros(2, batch_size, self.in_dim//2).to(device),\n",
    "        )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "EspHhKOGdFWt",
    "outputId": "beda104e-d424-4464-9e81-b5094d80b896"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharLM(\n",
       "  (emb): TimeDistributed(\n",
       "    (mod): Embedding(52, 20)\n",
       "  )\n",
       "  (conv_layer): ConvLayer(\n",
       "    (convModule): ModuleList(\n",
       "      (0): TimeDistributed(\n",
       "        (mod): Conv1d(20, 40, kernel_size=(2,), stride=(1,))\n",
       "      )\n",
       "      (1): TimeDistributed(\n",
       "        (mod): Conv1d(20, 60, kernel_size=(3,), stride=(1,))\n",
       "      )\n",
       "      (2): TimeDistributed(\n",
       "        (mod): Conv1d(20, 80, kernel_size=(4,), stride=(1,))\n",
       "      )\n",
       "      (3): TimeDistributed(\n",
       "        (mod): Conv1d(20, 100, kernel_size=(5,), stride=(1,))\n",
       "      )\n",
       "      (4): TimeDistributed(\n",
       "        (mod): Conv1d(20, 120, kernel_size=(6,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (highway_layer): TimeDistributed(\n",
       "    (mod): HighWayNetwork(\n",
       "      (hx): Linear(in_features=400, out_features=400, bias=True)\n",
       "      (tx): Linear(in_features=400, out_features=400, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(400, 200, num_layers=2, bias=False, batch_first=True, dropout=0.2)\n",
       "  (classify): Linear(in_features=200, out_features=6023, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharLM(len(CHARS.vocab) , len(TARGETS.vocab), EMBEDDING_DIM)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pLYiRaitEQGY",
    "outputId": "e9fc5982-774a-45c3-b6d2-a8bdb7079c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params 2362840\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of params {sum([p.numel() for p in model.parameters() if p.requires_grad])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach().clone()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "voO96RFJAf4C",
    "outputId": "64ab4898-4a3c-47af-aaac-1275f8c69f41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:50<00:00, 45.98it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:23<00:00, 108.66it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:07,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "Train Loss: 5.399, Train PPL: 221.09\n",
      "Valid Loss: 5.255, Valid PPL: 191.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:51<00:00, 44.97it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:24<00:00, 105.46it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:14,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-02.\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 4.146, Train PPL: 63.20\n",
      "Valid Loss: 5.445, Valid PPL: 231.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:51<00:00, 44.88it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:23<00:00, 110.54it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:18,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     3: reducing learning rate of group 0 to 5.0000e-03.\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 3.446, Train PPL: 31.38\n",
      "Valid Loss: 5.471, Valid PPL: 237.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:51<00:00, 44.54it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:23<00:00, 109.48it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:13,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     4: reducing learning rate of group 0 to 5.0000e-04.\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 3.326, Train PPL: 27.82\n",
      "Valid Loss: 5.484, Valid PPL: 240.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:52<00:00, 44.17it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:23<00:00, 111.08it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:13,  9.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     5: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 3.312, Train PPL: 27.43\n",
      "Valid Loss: 5.484, Valid PPL: 240.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:51<00:00, 44.91it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:22<00:00, 112.18it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:06,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-06.\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 3.310, Train PPL: 27.40\n",
      "Valid Loss: 5.484, Valid PPL: 240.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:51<00:00, 45.06it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:24<00:00, 106.74it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:05,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-07.\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 3.310, Train PPL: 27.38\n",
      "Valid Loss: 5.484, Valid PPL: 240.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:51<00:00, 45.15it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:23<00:00, 109.33it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:20,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-08.\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 3.310, Train PPL: 27.39\n",
      "Valid Loss: 5.484, Valid PPL: 240.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:51<00:00, 44.57it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:23<00:00, 111.81it/s]\n",
      "Train:   0%|          | 1/2304 [00:00<04:17,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-09.\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 3.310, Train PPL: 27.38\n",
      "Valid Loss: 5.484, Valid PPL: 240.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2304/2304 [00:51<00:00, 45.10it/s]\n",
      "Valid: 100%|██████████| 2575/2575 [00:22<00:00, 112.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10\n",
      "Train Loss: 3.310, Train PPL: 27.38\n",
      "Valid Loss: 5.484, Valid PPL: 240.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=INIT_LR,momentum=0.9,nesterov=True, weight_decay=1e-5)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, threshold=SCHEDULER_THRESHOLD, threshold_mode='abs', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE, verbose=True)\n",
    "\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    h = model.init_hidden(BATCH_SIZE)\n",
    "    for batch in tqdm(train_iter, desc='Train'):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if batch.chars.size(0) != h[0].shape[1]:\n",
    "            h = model.init_hidden(batch.chars.size(0))\n",
    "        h = repackage_hidden(h) # detach the tensor otherwise it will backpropagate till the entire dataset.\n",
    "        predictions, h = model(batch.chars, h)\n",
    "\n",
    "        loss = criterion(predictions, batch.targets.view(-1,))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #calculate metrics averaged across whole epoch\n",
    "    train_loss = epoch_loss / len(train_iter)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    val_h = model.init_hidden(BATCH_SIZE)\n",
    "    for batch in tqdm(valid_iter, desc='Valid'):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if batch.chars.size(0) != val_h[0].shape[1]:\n",
    "                val_h = model.init_hidden(batch.chars.size(0))\n",
    "            predictions, val_h = model(batch.chars,val_h)\n",
    "\n",
    "            loss = criterion(predictions, batch.targets.view(-1,))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    #calculate metrics averaged across whole epoch\n",
    "    valid_loss = epoch_loss / len(valid_iter)\n",
    "\n",
    "    #update learning rate\n",
    "    scheduler.step(math.exp(valid_loss))\n",
    "\n",
    "    #print metrics\n",
    "    print(f'\\nEpoch: {epoch}') \n",
    "    print(f'Train Loss: {train_loss:.3f}, Train PPL: {math.exp(train_loss):.2f}')\n",
    "    print(f'Valid Loss: {valid_loss:.3f}, Valid PPL: {math.exp(valid_loss):.2f}')\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ELMo (Embeddings from Language Model)](https://arxiv.org/pdf/1802.05365.pdf)\n",
    "\n",
    "The point where word2vec, Glove and fasttext failed is, The meaning, semantics of a words changes with respect to the words it is surrounded by in a sentence and these models wheren't able to represent that.\n",
    "\n",
    "For example:- \n",
    "\"\n",
    "1. Apple is in profit.\n",
    "2. Apple is tasty.\n",
    "\n",
    "Apple in first sentence refers to the company whereas apple in second sentence refers to fruit.\n",
    "\n",
    "In word2vec model this one word with two meaning will be represented by same vector.\n",
    "\n",
    "To solve this issue ELMo uses stacked bi-LSTM to generate embeddings for each words which is context dependent.\n",
    "\n",
    "so In the above example Apple in first and second case will have two different vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding ELMO\n",
    "\n",
    "Read Section 3.1 to understand ELMO implementation for details. In short, It does forward and Backward Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multi-layer Bi-LSTM take independent words as input and the hidden state of the LSTM is the output embedding representing the word.\n",
    "\n",
    "But Before passing the words as input to BI-LSTM the words are passed throught a char-CNN model to generate a fixed representation of a the word.\n",
    "\n",
    "Since we are using bidirectional LSTM so the forward and backward output of LSTM is returned as the embeddings for that particular word.\n",
    "\n",
    "output = [forward_representaion,backward_representation]\n",
    "\n",
    "we can average the two representation to get one representation of the word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMO Application\n",
    "\n",
    "Since ELMo embeddings are trained in a unsupervised way. These embeddings can be used for downstream task in supervised way.\n",
    "\n",
    "To add ELMo in a supervised task. First Freeze the bi-LSTM layer and generate ELMo embeddings concatenate it with x and pass it to the the task RNN.\n",
    "\n",
    "output = [x,ELMo]\n",
    "\n",
    "where x is the normal context-independent token representation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ELMO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
